{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bac129b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28d059380f74d86941a9799a444b43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/977 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970f08e04e584bb69503ea7618a07d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/111M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b972a4fecb14a98b5336f2951ae1f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/111M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import ConvNextForImageClassification\n",
    "\n",
    "model = ConvNextForImageClassification.from_pretrained(\n",
    "    \"ahsanjavid/convnext-tiny-finetuned-cifar10\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5866229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ConvNextForImageClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "def load_hf_convnext_backbone(hf_repo=\"ahsanjavid/convnext-tiny-finetuned-cifar10\", to_gray=True):\n",
    "    model = ConvNextForImageClassification.from_pretrained(hf_repo)\n",
    "    backbone = model.convnext  # Extract the backbone (without the classifier head)\n",
    "\n",
    "    if to_gray:\n",
    "        # Convert first conv layer from (3,...) → (1,...)\n",
    "        old_conv = backbone.embeddings.patch_embeddings\n",
    "        new_conv = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=old_conv.out_channels,\n",
    "            kernel_size=old_conv.kernel_size,\n",
    "            stride=old_conv.stride,\n",
    "            padding=old_conv.padding,\n",
    "            bias=old_conv.bias is not None,\n",
    "        )\n",
    "\n",
    "        # Average RGB weights to grayscale\n",
    "        with torch.no_grad():\n",
    "            new_conv.weight[:] = old_conv.weight.mean(dim=1, keepdim=True)\n",
    "            if old_conv.bias is not None:\n",
    "                new_conv.bias[:] = old_conv.bias\n",
    "        backbone.embeddings.patch_embeddings = new_conv\n",
    "\n",
    "    return backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4614aadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNextModel(\n",
       "  (embeddings): ConvNextEmbeddings(\n",
       "    (patch_embeddings): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (layernorm): ConvNextLayerNorm()\n",
       "  )\n",
       "  (encoder): ConvNextEncoder(\n",
       "    (stages): ModuleList(\n",
       "      (0): ConvNextStage(\n",
       "        (downsampling_layer): Identity()\n",
       "        (layers): Sequential(\n",
       "          (0): ConvNextLayer(\n",
       "            (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNextLayer(\n",
       "            (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNextLayer(\n",
       "            (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ConvNextStage(\n",
       "        (downsampling_layer): Sequential(\n",
       "          (0): ConvNextLayerNorm()\n",
       "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): Sequential(\n",
       "          (0): ConvNextLayer(\n",
       "            (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNextLayer(\n",
       "            (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNextLayer(\n",
       "            (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ConvNextStage(\n",
       "        (downsampling_layer): Sequential(\n",
       "          (0): ConvNextLayerNorm()\n",
       "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): Sequential(\n",
       "          (0): ConvNextLayer(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNextLayer(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNextLayer(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (3): ConvNextLayer(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (4): ConvNextLayer(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (5): ConvNextLayer(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (6): ConvNextLayer(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (7): ConvNextLayer(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (8): ConvNextLayer(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ConvNextStage(\n",
       "        (downsampling_layer): Sequential(\n",
       "          (0): ConvNextLayerNorm()\n",
       "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): Sequential(\n",
       "          (0): ConvNextLayer(\n",
       "            (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNextLayer(\n",
       "            (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNextLayer(\n",
       "            (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_hf_convnext_backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a569c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 All ConvNeXt models:\n",
      "  - convnext_atto\n",
      "  - convnext_atto_ols\n",
      "  - convnext_atto_rms\n",
      "  - convnext_base\n",
      "  - convnext_femto\n",
      "  - convnext_femto_ols\n",
      "  - convnext_large\n",
      "  - convnext_large_mlp\n",
      "  - convnext_nano\n",
      "  - convnext_nano_ols\n",
      "  - convnext_pico\n",
      "  - convnext_pico_ols\n",
      "  - convnext_small\n",
      "  - convnext_tiny\n",
      "  - convnext_tiny_hnf\n",
      "  - convnext_xlarge\n",
      "  - convnext_xxlarge\n",
      "  - convnext_zepto_rms\n",
      "  - convnext_zepto_rms_ols\n",
      "  - convnextv2_atto\n",
      "  - convnextv2_base\n",
      "  - convnextv2_femto\n",
      "  - convnextv2_huge\n",
      "  - convnextv2_large\n",
      "  - convnextv2_nano\n",
      "  - convnextv2_pico\n",
      "  - convnextv2_small\n",
      "  - convnextv2_tiny\n",
      "\n",
      "✅ Pretrained ConvNeXt models:\n",
      "  - convnext_atto.d2_in1k\n",
      "  - convnext_atto_ols.a2_in1k\n",
      "  - convnext_base.clip_laion2b\n",
      "  - convnext_base.clip_laion2b_augreg\n",
      "  - convnext_base.clip_laion2b_augreg_ft_in1k\n",
      "  - convnext_base.clip_laion2b_augreg_ft_in12k\n",
      "  - convnext_base.clip_laion2b_augreg_ft_in12k_in1k\n",
      "  - convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384\n",
      "  - convnext_base.clip_laiona\n",
      "  - convnext_base.clip_laiona_320\n",
      "  - convnext_base.clip_laiona_augreg_320\n",
      "  - convnext_base.clip_laiona_augreg_ft_in1k_384\n",
      "  - convnext_base.fb_in1k\n",
      "  - convnext_base.fb_in22k\n",
      "  - convnext_base.fb_in22k_ft_in1k\n",
      "  - convnext_base.fb_in22k_ft_in1k_384\n",
      "  - convnext_femto.d1_in1k\n",
      "  - convnext_femto_ols.d1_in1k\n",
      "  - convnext_large.fb_in1k\n",
      "  - convnext_large.fb_in22k\n",
      "  - convnext_large.fb_in22k_ft_in1k\n",
      "  - convnext_large.fb_in22k_ft_in1k_384\n",
      "  - convnext_large_mlp.clip_laion2b_augreg\n",
      "  - convnext_large_mlp.clip_laion2b_augreg_ft_in1k\n",
      "  - convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384\n",
      "  - convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384\n",
      "  - convnext_large_mlp.clip_laion2b_ft_320\n",
      "  - convnext_large_mlp.clip_laion2b_ft_soup_320\n",
      "  - convnext_large_mlp.clip_laion2b_soup_ft_in12k_320\n",
      "  - convnext_large_mlp.clip_laion2b_soup_ft_in12k_384\n",
      "  - convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320\n",
      "  - convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384\n",
      "  - convnext_nano.d1h_in1k\n",
      "  - convnext_nano.in12k\n",
      "  - convnext_nano.in12k_ft_in1k\n",
      "  - convnext_nano.r384_ad_in12k\n",
      "  - convnext_nano.r384_in12k\n",
      "  - convnext_nano.r384_in12k_ft_in1k\n",
      "  - convnext_nano_ols.d1h_in1k\n",
      "  - convnext_pico.d1_in1k\n",
      "  - convnext_pico_ols.d1_in1k\n",
      "  - convnext_small.fb_in1k\n",
      "  - convnext_small.fb_in22k\n",
      "  - convnext_small.fb_in22k_ft_in1k\n",
      "  - convnext_small.fb_in22k_ft_in1k_384\n",
      "  - convnext_small.in12k\n",
      "  - convnext_small.in12k_ft_in1k\n",
      "  - convnext_small.in12k_ft_in1k_384\n",
      "  - convnext_tiny.fb_in1k\n",
      "  - convnext_tiny.fb_in22k\n",
      "  - convnext_tiny.fb_in22k_ft_in1k\n",
      "  - convnext_tiny.fb_in22k_ft_in1k_384\n",
      "  - convnext_tiny.in12k\n",
      "  - convnext_tiny.in12k_ft_in1k\n",
      "  - convnext_tiny.in12k_ft_in1k_384\n",
      "  - convnext_tiny_hnf.a2h_in1k\n",
      "  - convnext_xlarge.fb_in22k\n",
      "  - convnext_xlarge.fb_in22k_ft_in1k\n",
      "  - convnext_xlarge.fb_in22k_ft_in1k_384\n",
      "  - convnext_xxlarge.clip_laion2b_rewind\n",
      "  - convnext_xxlarge.clip_laion2b_soup\n",
      "  - convnext_xxlarge.clip_laion2b_soup_ft_in1k\n",
      "  - convnext_xxlarge.clip_laion2b_soup_ft_in12k\n",
      "  - convnext_zepto_rms.ra4_e3600_r224_in1k\n",
      "  - convnext_zepto_rms_ols.ra4_e3600_r224_in1k\n",
      "  - convnextv2_atto.fcmae\n",
      "  - convnextv2_atto.fcmae_ft_in1k\n",
      "  - convnextv2_base.fcmae\n",
      "  - convnextv2_base.fcmae_ft_in1k\n",
      "  - convnextv2_base.fcmae_ft_in22k_in1k\n",
      "  - convnextv2_base.fcmae_ft_in22k_in1k_384\n",
      "  - convnextv2_femto.fcmae\n",
      "  - convnextv2_femto.fcmae_ft_in1k\n",
      "  - convnextv2_huge.fcmae\n",
      "  - convnextv2_huge.fcmae_ft_in1k\n",
      "  - convnextv2_huge.fcmae_ft_in22k_in1k_384\n",
      "  - convnextv2_huge.fcmae_ft_in22k_in1k_512\n",
      "  - convnextv2_large.fcmae\n",
      "  - convnextv2_large.fcmae_ft_in1k\n",
      "  - convnextv2_large.fcmae_ft_in22k_in1k\n",
      "  - convnextv2_large.fcmae_ft_in22k_in1k_384\n",
      "  - convnextv2_nano.fcmae\n",
      "  - convnextv2_nano.fcmae_ft_in1k\n",
      "  - convnextv2_nano.fcmae_ft_in22k_in1k\n",
      "  - convnextv2_nano.fcmae_ft_in22k_in1k_384\n",
      "  - convnextv2_pico.fcmae\n",
      "  - convnextv2_pico.fcmae_ft_in1k\n",
      "  - convnextv2_tiny.fcmae\n",
      "  - convnextv2_tiny.fcmae_ft_in1k\n",
      "  - convnextv2_tiny.fcmae_ft_in22k_in1k\n",
      "  - convnextv2_tiny.fcmae_ft_in22k_in1k_384\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "# List all ConvNeXt model names\n",
    "all_models = timm.list_models('convnext*')\n",
    "print(\"🔍 All ConvNeXt models:\")\n",
    "for name in all_models:\n",
    "    print(\"  -\", name)\n",
    "\n",
    "# List only pretrained ConvNeXt models\n",
    "pretrained_models = timm.list_models('convnext*', pretrained=True)\n",
    "print(\"\\n✅ Pretrained ConvNeXt models:\")\n",
    "for name in pretrained_models:\n",
    "    print(\"  -\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e81362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: convnext_tiny, pretrained: False\n"
     ]
    }
   ],
   "source": [
    "# backbone=\"convnext_fb_in22k_ft_in1k\"\n",
    "# backbone=\"convnext_fb_in1k\"\n",
    "backbone=\"convnext_gaussian\"\n",
    "if backbone.startswith('convnext'):\n",
    "    tokens = backbone.split('_')\n",
    "    init = tokens[-1] if len(tokens) > 1 else 'fb_in1k'\n",
    "\n",
    "    if init == 'fb_in22k_ft_in1k':\n",
    "        model_name = 'convnext_tiny.fb_in22k_ft_in1k'\n",
    "        pretrained = True\n",
    "    elif init == 'fb_in1k':\n",
    "        model_name = 'convnext_tiny.fb_in1k'\n",
    "        pretrained = True\n",
    "    else:\n",
    "        model_name = 'convnext_tiny'\n",
    "        pretrained = False\n",
    "print(f\"Using model: {model_name}, pretrained: {pretrained}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
