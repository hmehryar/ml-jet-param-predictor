{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /home/arsalan/wsu-grid/ml-jet-param-predictor\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Get the absolute path of the parent folder (where config.py lives)\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "print(\"Added to sys.path:\", parent_dir)\n",
    "\n",
    "# now your imports should work\n",
    "from config import get_config\n",
    "from train_utils.gpu_utils import get_device_summary\n",
    "from data.loader import get_dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "from config import get_config\n",
    "from train_utils.gpu_utils import get_device_summary\n",
    "from data.loader import get_dataloaders\n",
    "from models.model import create_model\n",
    "from train_utils.resume import init_resume_state\n",
    "from train_utils.resume import fill_trackers_from_history\n",
    "from train_utils.resume import load_pretrained_model\n",
    "from train_utils.training_loop import run_training_loop\n",
    "from train_utils.scheduler_utils import create_scheduler\n",
    "from train_utils.training_summary import finalize_training_summary\n",
    "from train_utils.training_summary import print_best_model_summary\n",
    "from train_utils.plot_metrics import plot_train_val_metrics\n",
    "from train_utils.plot_metrics import plot_loss_accuracy\n",
    "from train_utils.plot_metrics import plot_confusion_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Config Path: /home/arsalan/wsu-grid/ml-jet-param-predictor/config/swin_bs32_ep50_lr1e-04_ds7200000_g500.yml\n",
      "[INFO] Detected native Ubuntu host: DS044955\n",
      "[INFO] Using dataset root: /home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_7200000_balanced_unshuffled\n",
      "[INFO] Extracted dataset_size from path: 7200000\n",
      "{\n",
      "  \"model_tag\": \"Swin_g500\",\n",
      "  \"backbone\": \"swin\",\n",
      "  \"batch_size\": 32,\n",
      "  \"epochs\": 50,\n",
      "  \"learning_rate\": 0.0001,\n",
      "  \"patience\": 10,\n",
      "  \"input_shape\": [\n",
      "    1,\n",
      "    32,\n",
      "    32\n",
      "  ],\n",
      "  \"global_max\": 121.79151153564453,\n",
      "  \"dataset_root_dir\": \"/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_7200000_balanced_unshuffled\",\n",
      "  \"train_csv\": \"/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_7200000_balanced_unshuffled/file_labels_aggregated_ds7200000_g500_train.csv\",\n",
      "  \"val_csv\": \"/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_7200000_balanced_unshuffled/file_labels_aggregated_ds7200000_g500_val.csv\",\n",
      "  \"test_csv\": \"/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_7200000_balanced_unshuffled/file_labels_aggregated_ds7200000_g500_test.csv\",\n",
      "  \"output_dir\": \"training_output/Swin_g500_bs32_ep50_lr1e-04_ds7200000_g500_sched_ReduceLROnPlateau\",\n",
      "  \"group_size\": 500,\n",
      "  \"scheduler\": {\n",
      "    \"type\": \"ReduceLROnPlateau\",\n",
      "    \"mode\": \"max\",\n",
      "    \"factor\": 0.5,\n",
      "    \"patience\": 4,\n",
      "    \"verbose\": true\n",
      "  },\n",
      "  \"dataset_size\": \"7200000\",\n",
      "  \"preload_model_path\": null,\n",
      "  \"loss_weights\": {\n",
      "    \"energy_loss_output\": 1.0,\n",
      "    \"alpha_output\": 1.0,\n",
      "    \"q0_output\": 1.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# cfg=get_config(config_path=\"/home/arsalan/wsu-grid/ml-jet-param-predictor/config/convnext_gaussian_bs32_ep50_lr1e-04_ds7200000_g500_sched-RLRP.yml\")\n",
    "# cfg=get_config(config_path=\"/home/arsalan/wsu-grid/ml-jet-param-predictor/config/efficientnet_bs32_ep50_lr1e-02_ds7200000_g500.yml\")\n",
    "\n",
    "cfg=get_config(config_path=\"/home/arsalan/wsu-grid/ml-jet-param-predictor/config/swin_bs32_ep50_lr1e-04_ds7200000_g500.yml\")\n",
    "# from models.model_vit import create_model\n",
    "# cfg=get_config(config_path=\"/home/arsalan/wsu-grid/ml-jet-param-predictor/\" \\\n",
    "# \"experiments/exp_preload_trained_model_and_train_more/config/vit_tiny_patch16_224_gaussian_bs32_ep50_lr1e-04_p12_ds7200000_g500_sched-RLRP_preload_p4.yml\")\n",
    "\n",
    "# from models.model_mamba import create_model\n",
    "# cfg=get_config(config_path=\"/home/arsalan/wsu-grid/ml-jet-param-predictor/\" \\\n",
    "# \"experiments/exp_preload_trained_model_and_train_more/config/mambaout_base_plus_rw_bs16_ep50_lr1e-04_p12_ds7200000_g500_sched-RLRP_preload-p4.yml\")\n",
    "\n",
    "# cfg=get_config()\n",
    "print(json.dumps(vars(cfg), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving all outputs to: training_output/Swin_g500_bs32_ep50_lr1e-04_ds7200000_g500_sched_ReduceLROnPlateau\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "print(f\"[INFO] Saving all outputs to: {cfg.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NVIDIA-SMI ===\n",
      "Fri Sep  5 18:44:28 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070        Off | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   37C    P5              24W / 220W |   1753MiB /  8192MiB |     23%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3129      G   /usr/lib/xorg/Xorg                          659MiB |\n",
      "|    0   N/A  N/A      3349      G   /usr/bin/gnome-shell                        187MiB |\n",
      "|    0   N/A  N/A      4164      G   /usr/libexec/xdg-desktop-portal-gnome         3MiB |\n",
      "|    0   N/A  N/A     20018      G   /snap/slack/212/usr/lib/slack/slack          84MiB |\n",
      "|    0   N/A  N/A     69578      G   ...seed-version=20250903-050052.843000      207MiB |\n",
      "|    0   N/A  N/A     82952      G   /proc/self/exe                              129MiB |\n",
      "|    0   N/A  N/A     83676      G   /usr/bin/nautilus                            30MiB |\n",
      "|    0   N/A  N/A     87255      G   ...irefox/6738/usr/lib/firefox/firefox       41MiB |\n",
      "|    0   N/A  N/A     90629      G   ...erProcess --variations-seed-version       32MiB |\n",
      "|    0   N/A  N/A     93221      C   .../miniconda3/envs/pytorch/bin/python      350MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n",
      "[INFO] PyTorch version:       2.5.1\n",
      "[INFO] Using device: cuda\n",
      "[INFO] CUDA available:        True\n",
      "[INFO] CUDA toolkit version:  12.1\n",
      "[INFO] Device count:          1\n",
      "[INFO] Number of CUDA devices: 1\n",
      "  - CUDA:0 â€” NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "device= get_device_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadClassifier(\n",
       "  (backbone): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): Sequential(\n",
       "      (0): SwinTransformerStage(\n",
       "        (downsample): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.009)\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.009)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.018)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.018)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.027)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.027)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.036)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.036)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.045)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.045)\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.055)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.055)\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.064)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.064)\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.073)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.073)\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.082)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.082)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.091)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.100)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): ClassifierHead(\n",
       "      (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (fc): Identity()\n",
       "      (flatten): Identity()\n",
       "    )\n",
       "  )\n",
       "  (features): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): Sequential(\n",
       "      (0): SwinTransformerStage(\n",
       "        (downsample): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.009)\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.009)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.018)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.018)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.027)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.027)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.036)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.036)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.045)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.045)\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.055)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.055)\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.064)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.064)\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.073)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.073)\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.082)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.082)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.091)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.100)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): ClassifierHead(\n",
       "      (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (fc): Identity()\n",
       "      (flatten): Identity()\n",
       "    )\n",
       "  )\n",
       "  (energy_loss_head): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (alpha_head): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (q0_head): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model and optimizer\n",
    "model, optimizer = create_model(cfg.backbone, cfg.input_shape, cfg.learning_rate)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 5,525,960\n",
      "Total parameters: 5,525,960\n"
     ]
    }
   ],
   "source": [
    "# # Count trainable parameters\n",
    "# num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"Trainable parameters: {num_params:,}\")\n",
    "\n",
    "# # (Optional) Count all parameters including frozen ones\n",
    "# num_params_total = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Total parameters: {num_params_total:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FLOPs / Params utilities =====\n",
    "# Drop this in a cell after you've created the model\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def _make_dummy_input(input_shape, device):\n",
    "    # input_shape: (C, H, W)\n",
    "    return torch.randn(1, *input_shape, device=device)\n",
    "\n",
    "def count_trainable_params(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def profile_with_fvcore(model, input_shape, device):\n",
    "    try:\n",
    "        from fvcore.nn import FlopCountAnalysis, parameter_count\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    model_eval = model.eval()\n",
    "    dummy = _make_dummy_input(input_shape, device)\n",
    "    with torch.no_grad():\n",
    "        flops = FlopCountAnalysis(model_eval, dummy).total()        # FLOPs (float ops)\n",
    "    params = parameter_count(model_eval)[\"\"]\n",
    "    macs = flops / 2.0                                              # approx: 1 MAC â‰ˆ 2 FLOPs\n",
    "    return dict(params=params, macs=macs, flops=flops)\n",
    "\n",
    "def profile_with_ptflops(model, input_shape, device):\n",
    "    try:\n",
    "        from ptflops import get_model_complexity_info\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    C, H, W = input_shape\n",
    "    # ptflops expects (C, H, W) and returns MACs\n",
    "    model_eval = model.eval().to(device)\n",
    "    with torch.no_grad():\n",
    "        macs_str, params_str = get_model_complexity_info(\n",
    "            model_eval, (C, H, W), as_strings=True, print_per_layer_stat=False, verbose=False\n",
    "        )\n",
    "    # Parse strings like '12.34 M', '0.56 G'\n",
    "    def _to_num(s):\n",
    "        s = s.strip().upper().replace(' ', '')\n",
    "        if s.endswith('K'): return float(s[:-1]) * 1e3\n",
    "        if s.endswith('M'): return float(s[:-1]) * 1e6\n",
    "        if s.endswith('G'): return float(s[:-1]) * 1e9\n",
    "        return float(s)\n",
    "    macs = _to_num(macs_str)\n",
    "    params = _to_num(params_str)\n",
    "    flops = macs * 2.0  # common convention\n",
    "    return dict(params=params, macs=macs, flops=flops)\n",
    "\n",
    "def profile_model(model, input_shape=(1,32,32), device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Returns a dict with:\n",
    "      - params (int)\n",
    "      - macs   (float, operations)\n",
    "      - flops  (float, operations)\n",
    "      - pretty strings for Params (M), MACs (G), FLOPs (G)\n",
    "    \"\"\"\n",
    "    model = model.to(device).eval()\n",
    "    # First: fvcore (FLOPs), else: ptflops (MACs)\n",
    "    result = profile_with_fvcore(model, input_shape, device)\n",
    "    if result is None:\n",
    "        result = profile_with_ptflops(model, input_shape, device)\n",
    "    if result is None:\n",
    "        raise RuntimeError(\"Neither fvcore nor ptflops is available. Install one of them to compute FLOPs/MACs.\")\n",
    "\n",
    "    params = int(result[\"params\"])\n",
    "    macs   = float(result[\"macs\"])\n",
    "    flops  = float(result[\"flops\"])\n",
    "\n",
    "    pretty = {\n",
    "        \"Params (M)\": f\"{params/1e6:.2f}\",\n",
    "        \"MACs (G)\" : f\"{macs/1e9:.3f}\",\n",
    "        \"FLOPs (G)\": f\"{flops/1e9:.3f}\",\n",
    "    }\n",
    "    return {\"params\": params, \"macs\": macs, \"flops\": flops, \"pretty\": pretty}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::rsub encountered 24 time(s)\n",
      "Unsupported operator aten::pad encountered 15 time(s)\n",
      "Unsupported operator aten::mul encountered 12 time(s)\n",
      "Unsupported operator aten::add encountered 37 time(s)\n",
      "Unsupported operator aten::softmax encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "backbone.layers.0.blocks.1.drop_path1, backbone.layers.0.blocks.1.drop_path2, backbone.layers.1.blocks.0.drop_path1, backbone.layers.1.blocks.0.drop_path2, backbone.layers.1.blocks.1.drop_path1, backbone.layers.1.blocks.1.drop_path2, backbone.layers.2.blocks.0.drop_path1, backbone.layers.2.blocks.0.drop_path2, backbone.layers.2.blocks.1.drop_path1, backbone.layers.2.blocks.1.drop_path2, backbone.layers.2.blocks.2.drop_path1, backbone.layers.2.blocks.2.drop_path2, backbone.layers.2.blocks.3.drop_path1, backbone.layers.2.blocks.3.drop_path2, backbone.layers.2.blocks.4.drop_path1, backbone.layers.2.blocks.4.drop_path2, backbone.layers.2.blocks.5.drop_path1, backbone.layers.2.blocks.5.drop_path2, backbone.layers.3.blocks.0.drop_path1, backbone.layers.3.blocks.0.drop_path2, backbone.layers.3.blocks.1.drop_path1, backbone.layers.3.blocks.1.drop_path2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 27,500,690\n",
      "All parameters: 27,500,690\n",
      "== Model profile ==\n",
      "Params (M): 27.50\n",
      "MACs (G): 0.045\n",
      "FLOPs (G): 0.090\n"
     ]
    }
   ],
   "source": [
    "# ===== Example usage after you build your model =====\n",
    "# model, optimizer = create_model(cfg.backbone, cfg.input_shape, cfg.learning_rate)\n",
    "# model.to(device)\n",
    "\n",
    "try:\n",
    "    prof = profile_model(model, input_shape=cfg.input_shape, device=device)\n",
    "    print(\"Trainable parameters:\", f\"{count_trainable_params(model):,}\")\n",
    "    print(\"All parameters:\", f\"{sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(\"== Model profile ==\")\n",
    "    for k, v in prof[\"pretty\"].items():\n",
    "        print(f\"{k}: {v}\")\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "    print(\"\\nQuick install (choose one):\")\n",
    "    print(\"  !pip install fvcore\")\n",
    "    print(\"  # or\")\n",
    "    print(\"  !pip install ptflops\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
