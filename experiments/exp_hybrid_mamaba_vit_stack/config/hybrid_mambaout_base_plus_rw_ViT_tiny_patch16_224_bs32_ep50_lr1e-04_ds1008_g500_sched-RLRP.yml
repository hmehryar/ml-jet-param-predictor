model_tag: mambaout_base_plus_rw_ViT_tiny_patch16_224_in1k_g500
backbone: [mambaout_base_plus_rw,vit_tiny_patch16_224]
batch_size: 32
epochs: 50
learning_rate: 0.0001
patience: 12
input_shape: [1, 32, 32]
global_max: 121.79151153564453
output_dir: training_output/
dataset_subdir: jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_7200000_balanced_unshuffled
group_size: 500
dataset_size: 1008
scheduler:
  type: ReduceLROnPlateau
  mode: max
  factor: 0.5
  patience: 4
  verbose: true