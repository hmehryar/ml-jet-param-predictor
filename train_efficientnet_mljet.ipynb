{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95811c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.val_val_val_metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "from data.loader import JetDataset, load_split_from_csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define all backbone-model_tag pairs to evaluate ===\n",
    "experiments = [\n",
    "    {\"model_tag\": \"EfficientNet\", \"backbone\": \"efficientnet\"},\n",
    "    {\"model_tag\": \"ConvNeXt\", \"backbone\": \"convnext\"},\n",
    "    {\"model_tag\": \"SwinTransformerV2\", \"backbone\": \"swin\"},\n",
    "    {\"model_tag\": \"Mamba\", \"backbone\": \"mamba\"},\n",
    "    {\"model_tag\": \"VisionMamba\", \"backbone\": \"vision_mamba\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ad3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_idx = 0\n",
    "model_tag = experiments[model_idx][\"model_tag\"]\n",
    "backbone = experiments[model_idx][\"backbone\"]\n",
    "# ==========================================================\n",
    "print(f\"Evaluating {model_tag} with backbone {backbone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3dccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "global_max = 121.79151153564453\n",
    "output_dir = 'training_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import getpass\n",
    "\n",
    "# === Determine platform-specific base path ===\n",
    "#--- System/User Info ---\n",
    "system = platform.system()\n",
    "release = platform.release().lower()\n",
    "hostname = socket.gethostname().lower()\n",
    "user = getpass.getuser().lower()\n",
    "\n",
    "# === Base path detection ===\n",
    "if system == \"Linux\" and \"wsl\" in platform.release().lower() and  \"arsalan\" in user:\n",
    "    base_path = \"/mnt/d/Projects/110_JetscapeML/hm_jetscapeml_source/data\"\n",
    "    print(\"[INFO] Detected WSL environment\")\n",
    "elif system == \"Linux\" and \"ds044955\" in hostname and \"arsalan\" in user:\n",
    "    base_path = \"/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data\"\n",
    "    print(\"[INFO] Detected native Ubuntu host: DS044955\")\n",
    "elif system == \"Linux\" and  \"gy4065\" in user:\n",
    "    base_path = \"/wsu/home/gy/gy40/gy4065/hm_jetscapeml_source/data\"\n",
    "    print(\"[INFO] Detected WSU Grid environment (user: gy4065)\")\n",
    "else:\n",
    "    raise RuntimeError(\"❌ Unknown system. Please define the dataset path for this host.\")\n",
    "\n",
    "# === Define dataset subdirectory (only this part changes per dataset) ===\n",
    "dataset_subdir = \"jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled\"\n",
    "\n",
    "# === Full dataset path ===\n",
    "dataset_root_dir = os.path.join(base_path, dataset_subdir)\n",
    "print(f\"[INFO] Using dataset root: {dataset_root_dir}\")\n",
    "\n",
    "# === Extract dataset size from path ===\n",
    "match = re.search(r\"size_(\\d+)\", dataset_root_dir)\n",
    "dataset_size = match.group(1) if match else \"unknown\"\n",
    "print(f\"[INFO] Detected dataset size: {dataset_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dynamic output directory\n",
    "run_tag = f\"{model_tag}_bs{batch_size}_ep{epochs}_lr{learning_rate:.0e}_ds{dataset_size}\"\n",
    "output_dir = os.path.join(output_dir, run_tag)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"[INFO] Saving all outputs to: {output_dir}\")\n",
    "\n",
    "train_csv = os.path.join(dataset_root_dir, 'train_files.csv')\n",
    "val_csv = os.path.join(dataset_root_dir, 'val_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splits from CSV\n",
    "train_list = load_split_from_csv(train_csv, dataset_root_dir)\n",
    "val_list = load_split_from_csv(val_csv, dataset_root_dir)\n",
    "\n",
    "train_dataset = JetDataset(train_list, global_max=global_max)\n",
    "val_dataset = JetDataset(val_list, global_max=global_max)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"[INFO] Training samples: {len(train_dataset)}\")\n",
    "print(f\"[INFO] Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "#show the length of the dataloaders\n",
    "print(f\"[INFO] Length of training dataloader: {len(train_loader)}\")\n",
    "print(f\"[INFO] Length of validation dataloader: {len(val_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e838b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "model, optimizer = create_model(backbone=backbone, input_shape=(1, 32, 32), learning_rate=learning_rate)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = {\n",
    "    'energy_loss_output': nn.BCELoss(),\n",
    "    'alpha_output': nn.CrossEntropyLoss(),\n",
    "    'q0_output': nn.CrossEntropyLoss()\n",
    "}\n",
    "print(criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_energy_list, train_loss_alpha_list, train_loss_q0_list, train_losses = [], [], [],[]\n",
    "train_energy_acc_list, train_alpha_acc_list, train_q0_acc_list, train_total_acc_list = [], [], [], []\n",
    "\n",
    "\n",
    "val_losses,val_loss_energy_list, val_loss_alpha_list,val_loss_q0_list = [], [], [], []\n",
    "acc_energy_list, acc_alpha_list,acc_q0_list ,acc_total_list = [],[],[],[]\n",
    "\n",
    "\n",
    "all_epoch_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af732c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    loss_energy_total = 0.0\n",
    "    loss_alpha_total = 0.0\n",
    "    loss_q0_total = 0.0\n",
    "    \n",
    "    correct_energy = 0\n",
    "    correct_alpha = 0\n",
    "    correct_q0 = 0\n",
    "    correct_all = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x = x.to(device)\n",
    "        for key in labels:\n",
    "            labels[key] = labels[key].to(device)\n",
    "         # Forward\n",
    "        outputs = model(x)\n",
    "        energy_out = outputs['energy_loss_output'].squeeze()\n",
    "        alpha_out = outputs['alpha_output']\n",
    "        q0_out = outputs['q0_output']\n",
    "\n",
    "        # Labels\n",
    "        gt_energy = labels['energy_loss_output'].squeeze()\n",
    "        gt_alpha = labels['alpha_output'].squeeze()\n",
    "        gt_q0 = labels['q0_output'].squeeze()\n",
    "\n",
    "        # Loss\n",
    "        loss_energy = criterion['energy_loss_output'](energy_out, gt_energy.float())\n",
    "        loss_alpha = criterion['alpha_output'](alpha_out, gt_alpha)\n",
    "        loss_q0 = criterion['q0_output'](q0_out, gt_q0)\n",
    "        total_batch_loss = loss_energy + loss_alpha + loss_q0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # running_loss += loss.item()\n",
    "        total_loss += total_batch_loss.item()\n",
    "        loss_energy_total += loss_energy.item()\n",
    "        loss_alpha_total += loss_alpha.item()\n",
    "        loss_q0_total += loss_q0.item()\n",
    "\n",
    "        # Accuracy\n",
    "        pred_energy = (energy_out > 0.5).long()\n",
    "        pred_alpha = torch.argmax(alpha_out, dim=1)\n",
    "        pred_q0 = torch.argmax(q0_out, dim=1)\n",
    "\n",
    "        correct_energy += (pred_energy == gt_energy).sum().item()\n",
    "        correct_alpha += (pred_alpha == gt_alpha).sum().item()\n",
    "        correct_q0 += (pred_q0 == gt_q0).sum().item()\n",
    "\n",
    "        correct_all += ((pred_energy == gt_energy) &\n",
    "                        (pred_alpha == gt_alpha) &\n",
    "                        (pred_q0 == gt_q0)).sum().item()\n",
    "\n",
    "        total += x.size(0)\n",
    "\n",
    "    return {\n",
    "        'train_loss': total_loss / len(loader),\n",
    "        'train_loss_energy': loss_energy_total / len(loader),\n",
    "        'train_loss_alpha': loss_alpha_total / len(loader),\n",
    "        'train_loss_q0': loss_q0_total / len(loader),\n",
    "        'train_acc_energy': correct_energy / total,\n",
    "        'train_acc_alpha': correct_alpha / total,\n",
    "        'train_acc_q0': correct_q0 / total,\n",
    "        'train_acc_total': correct_all / total\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ccf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    y_true = {'energy': [], 'alpha': [], 'q0': []}\n",
    "    y_pred = {'energy': [], 'alpha': [], 'q0': []}\n",
    "    correct_all = 0\n",
    "    total = 0\n",
    "\n",
    "    val_loss_total = 0.0\n",
    "    val_loss_energy = 0.0\n",
    "    val_loss_alpha = 0.0\n",
    "    val_loss_q0 = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, labels in loader:\n",
    "            x = x.to(device)\n",
    "            for key in labels:\n",
    "                labels[key] = labels[key].to(device)\n",
    "\n",
    "            outputs = model(x)\n",
    "\n",
    "            # Energy loss: binary thresholding\n",
    "            pred_energy = (outputs['energy_loss_output'] > 0.5).long().squeeze()\n",
    "            pred_alpha = torch.argmax(outputs['alpha_output'], dim=1)\n",
    "            pred_q0 = torch.argmax(outputs['q0_output'], dim=1)\n",
    "\n",
    "            gt_energy = labels['energy_loss_output'].squeeze()\n",
    "            gt_alpha = labels['alpha_output'].squeeze()\n",
    "            gt_q0 = labels['q0_output'].squeeze()\n",
    "\n",
    "            y_true['energy'].extend(labels['energy_loss_output'].squeeze().cpu().numpy())\n",
    "            y_true['alpha'].extend(labels['alpha_output'].squeeze().cpu().numpy())\n",
    "            y_true['q0'].extend(labels['q0_output'].squeeze().cpu().numpy())\n",
    "\n",
    "            y_pred['energy'].extend(pred_energy.cpu().numpy())\n",
    "            y_pred['alpha'].extend(pred_alpha.cpu().numpy())\n",
    "            y_pred['q0'].extend(pred_q0.cpu().numpy())\n",
    "\n",
    "            # Total accuracy = all 3 correct\n",
    "            correct_batch = ((pred_energy == gt_energy) &\n",
    "                             (pred_alpha == gt_alpha) &\n",
    "                             (pred_q0 == gt_q0)).sum().item()\n",
    "            correct_all += correct_batch\n",
    "            total += x.size(0)\n",
    "\n",
    "            # Compute loss per task\n",
    "            energy_out = outputs['energy_loss_output'].squeeze()\n",
    "            alpha_out = outputs['alpha_output']\n",
    "            q0_out = outputs['q0_output']\n",
    "\n",
    "            loss_energy = criterion['energy_loss_output'](energy_out, gt_energy.float())\n",
    "            loss_alpha = criterion['alpha_output'](alpha_out, gt_alpha)\n",
    "            loss_q0 = criterion['q0_output'](q0_out, gt_q0)\n",
    "\n",
    "            val_loss_energy += loss_energy.item()\n",
    "            val_loss_alpha += loss_alpha.item()\n",
    "            val_loss_q0 += loss_q0.item()\n",
    "            val_loss_total += (loss_energy + loss_alpha + loss_q0).item()\n",
    "\n",
    "            \n",
    "    # Compute individual accuracies\n",
    "    acc_total = correct_all / total\n",
    "    avg_loss_energy = val_loss_energy / len(loader)\n",
    "    avg_loss_alpha = val_loss_alpha / len(loader)\n",
    "    avg_loss_q0 = val_loss_q0 / len(loader)\n",
    "    avg_val_loss = val_loss_total / len(loader)\n",
    "    \n",
    "    # All val_metrics + losses in one dict\n",
    "    metrics = {\n",
    "        \"total_accuracy\": acc_total,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"val_loss_energy\": avg_loss_energy,\n",
    "        \"val_loss_alpha\": avg_loss_alpha,\n",
    "        \"val_loss_q0\": avg_loss_q0,\n",
    "        \"energy\": {\n",
    "            \"accuracy\": accuracy_score(y_true['energy'], y_pred['energy']),\n",
    "            \"precision\": precision_score(y_true['energy'], y_pred['energy'], average='macro'),\n",
    "            \"recall\": recall_score(y_true['energy'], y_pred['energy'], average='macro'),\n",
    "            \"f1\": f1_score(y_true['energy'], y_pred['energy'], average='macro'),\n",
    "        },\n",
    "        \"alpha\": {\n",
    "            \"accuracy\": accuracy_score(y_true['alpha'], y_pred['alpha']),\n",
    "            \"precision\": precision_score(y_true['alpha'], y_pred['alpha'], average='macro'),\n",
    "            \"recall\": recall_score(y_true['alpha'], y_pred['alpha'], average='macro'),\n",
    "            \"f1\": f1_score(y_true['alpha'], y_pred['alpha'], average='macro'),\n",
    "        },\n",
    "        \"q0\": {\n",
    "            \"accuracy\": accuracy_score(y_true['q0'], y_pred['q0']),\n",
    "            \"precision\": precision_score(y_true['q0'], y_pred['q0'], average='macro'),\n",
    "            \"recall\": recall_score(y_true['q0'], y_pred['q0'], average='macro'),\n",
    "            \"f1\": f1_score(y_true['q0'], y_pred['q0'], average='macro'),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee2f0ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected WSL environment\n",
      "[INFO] Using dataset root: /mnt/d/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled\n",
      "[INFO] Detected dataset size: 1000\n",
      "[INFO] model tag: EfficientNet, backbone: efficientnet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "namespace(model_tag='EfficientNet',\n",
       "          backbone='efficientnet',\n",
       "          batch_size=512,\n",
       "          epochs=50,\n",
       "          learning_rate=0.0001,\n",
       "          patience=5,\n",
       "          input_shape=(1, 32, 32),\n",
       "          global_max=121.79151153564453,\n",
       "          dataset_root_dir='/mnt/d/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled',\n",
       "          train_csv='/mnt/d/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled/train_files.csv',\n",
       "          val_csv='/mnt/d/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled/val_files.csv',\n",
       "          test_csv='/mnt/d/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled/test_files.csv',\n",
       "          output_dir='training_output/EfficientNet_bs512_ep50_lr1e-04_ds1000')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving all outputs to: training_output/EfficientNet_bs512_ep50_lr1e-04_ds1000\n",
      "[INFO] Training samples: 796\n",
      "[INFO] Validation samples: 96\n",
      "[INFO] Test samples: 108\n",
      "[INFO] Length of training dataloader: 2\n",
      "[INFO] Length of validation dataloader: 1\n",
      "[INFO] Length of test dataloader: 1\n",
      "[INFO] Loss functions:{'energy_loss_output': BCELoss(), 'alpha_output': CrossEntropyLoss(), 'q0_output': CrossEntropyLoss()}\n",
      "[INFO] Init Training Trackers\n",
      "[INFO] Init Validation Trackers\n",
      "[INFO] Init Resume/Training Parameters\n",
      "[INFO] Loading checkpoint if exists:TODO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsi/miniconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/arsi/miniconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/arsi/miniconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Epoch 1: Energy Acc =0.7500, αs Acc = 0.3333, Q0 Acc = 0.2500, Total Acc = 0.0833\n",
      "✅ Best model saved at epoch 1 with total accuracy: 0.0833\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 131\u001b[0m\n\u001b[1;32m    127\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(all_epoch_metrics, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    129\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_epoch_metrics)\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cfg\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_metrics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 131\u001b[0m all_epoch_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrecord_and_save_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_epoch_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wsu-grid/ml-jet-param-predictor/train_utils/train_metrics_logger.py:105\u001b[0m, in \u001b[0;36mrecord_and_save_epoch\u001b[0;34m(epoch, train_metrics, val_metrics, all_epoch_metrics, output_dir)\u001b[0m\n\u001b[1;32m    102\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(all_epoch_metrics, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[1;32m    106\u001b[0m     {\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_energy\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_energy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_q0\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_q0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_energy\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_energy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_q0\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_q0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_energy\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_energy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_q0\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_q0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_energy_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_energy_prec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_energy_rec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_energy_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_alpha_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_alpha_prec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_alpha_rec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_alpha_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_q0_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_q0_prec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_q0_rec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_q0_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    133\u001b[0m     }\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m all_epoch_metrics\n\u001b[1;32m    135\u001b[0m ])\n\u001b[1;32m    136\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_metrics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_epoch_metrics\n",
      "File \u001b[0;32m~/wsu-grid/ml-jet-param-predictor/train_utils/train_metrics_logger.py:120\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    102\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(all_epoch_metrics, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[1;32m    105\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[1;32m    106\u001b[0m     {\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_energy\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_energy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_q0\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_q0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_energy\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_energy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_q0\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc_q0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_energy\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_energy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_q0\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_q0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_energy_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_energy_prec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_energy_rec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_energy_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_alpha_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_alpha_prec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_alpha_rec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_alpha_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_q0_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_q0_prec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_q0_rec\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_q0_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    133\u001b[0m     }\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m all_epoch_metrics\n\u001b[1;32m    135\u001b[0m ])\n\u001b[1;32m    136\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_metrics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_epoch_metrics\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from config import get_config\n",
    "from data.loader import get_dataloaders\n",
    "from models.model import create_model\n",
    "from train_utils.train_epoch import train_one_epoch\n",
    "from train_utils.evaluate import evaluate\n",
    "from train_utils.train_metrics_logger import update_train_logs\n",
    "from train_utils.train_metrics_logger import update_val_logs\n",
    "from train_utils.train_metrics_logger import record_and_save_epoch\n",
    "\n",
    "cfg=get_config()\n",
    "display(cfg)\n",
    "\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "print(f\"[INFO] Saving all outputs to: {cfg.output_dir}\")\n",
    "\n",
    "\n",
    "# Set seed, device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data\n",
    "train_loader, val_loader, test_loader = get_dataloaders(cfg)\n",
    "\n",
    "# Model and optimizer\n",
    "model, optimizer = create_model(cfg.backbone, cfg.input_shape, cfg.learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "criterion = {\n",
    "    'energy_loss_output': nn.BCELoss(),\n",
    "    'alpha_output': nn.CrossEntropyLoss(),\n",
    "    'q0_output': nn.CrossEntropyLoss()\n",
    "}\n",
    "print(f\"[INFO] Loss functions:{criterion}\")\n",
    "\n",
    "print(f\"[INFO] Init Training Trackers\")\n",
    "train_loss_energy_list, train_loss_alpha_list, train_loss_q0_list, train_losses = [], [], [],[]\n",
    "train_energy_acc_list, train_alpha_acc_list, train_q0_acc_list, train_total_acc_list = [], [], [], []\n",
    "\n",
    "print(f\"[INFO] Init Validation Trackers\")\n",
    "val_losses,val_loss_energy_list, val_loss_alpha_list,val_loss_q0_list = [], [], [], []\n",
    "acc_energy_list, acc_alpha_list,acc_q0_list ,acc_total_list = [],[],[],[]\n",
    "\n",
    "print(f\"[INFO] Init Resume/Training Parameters\")\n",
    "early_stop_counter,start_epoch,best_total_acc,best_epoch,best_metrics,all_epoch_metrics= 0,0 , 0,0,{},[]\n",
    "\n",
    "print(f\"[INFO] Loading checkpoint if exists:TODO\")     \n",
    "\n",
    "for epoch in range(start_epoch, cfg.epochs):\n",
    "    train_metrics = train_one_epoch(train_loader, model, criterion, optimizer, device)\n",
    "    # display(f\"train metrics: {train_metrics}\")\n",
    "\n",
    "    val_metrics = evaluate(val_loader, model, criterion, device)\n",
    "    # display(f\"val metrics: {val_metrics}\")\n",
    "\n",
    "    print(\"=\"*cfg.epochs)\n",
    "    print(f\"Epoch {epoch+1}: Energy Acc ={val_metrics['energy']['accuracy']:.4f}, αs Acc = {val_metrics['alpha']['accuracy']:.4f}, Q0 Acc = {val_metrics['q0']['accuracy']:.4f}, Total Acc = {val_metrics['val_accuracy']:.4f}\")\n",
    "\n",
    "    (train_losses,\n",
    "    train_loss_energy_list,\n",
    "    train_loss_alpha_list,\n",
    "    train_loss_q0_list,\n",
    "    train_energy_acc_list,\n",
    "    train_alpha_acc_list,\n",
    "    train_q0_acc_list,\n",
    "    train_total_acc_list) = update_train_logs(\n",
    "        train_metrics,\n",
    "        train_losses,\n",
    "        train_loss_energy_list,\n",
    "        train_loss_alpha_list,\n",
    "        train_loss_q0_list,\n",
    "        train_energy_acc_list,\n",
    "        train_alpha_acc_list,\n",
    "        train_q0_acc_list,\n",
    "        train_total_acc_list\n",
    "    )\n",
    "    (val_losses,\n",
    "    val_loss_energy_list,\n",
    "    val_loss_alpha_list,\n",
    "    val_loss_q0_list,\n",
    "    acc_energy_list,\n",
    "    acc_alpha_list,\n",
    "    acc_q0_list,\n",
    "    acc_total_list) = update_val_logs(\n",
    "        val_metrics,\n",
    "        val_losses,\n",
    "        val_loss_energy_list,\n",
    "        val_loss_alpha_list,\n",
    "        val_loss_q0_list,\n",
    "        acc_energy_list,\n",
    "        acc_alpha_list,\n",
    "        acc_q0_list,\n",
    "        acc_total_list\n",
    "    )\n",
    "\n",
    "    if val_metrics[\"val_accuracy\"] > best_total_acc:\n",
    "        best_total_acc = val_metrics[\"val_accuracy\"]\n",
    "        best_metrics = val_metrics\n",
    "        best_epoch = epoch + 1\n",
    "        early_stop_counter = 0  # reset counter\n",
    "\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': best_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metrics': best_metrics,\n",
    "        }, os.path.join(cfg.output_dir, \"best_model.pth\"))\n",
    "\n",
    "        print(f\"✅ Best model saved at epoch {best_epoch} with total accuracy: {best_total_acc:.4f}\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"⏳ No improvement. Early stop counter: {early_stop_counter}/{cfg.patience}\")\n",
    "        if early_stop_counter >= cfg.patience:\n",
    "            print(f\"🛑 Early stopping triggered at epoch {epoch+1}. Best was at epoch {best_epoch}.\")\n",
    "            break\n",
    "    # Save epoch record\n",
    "    epoch_record = {\"epoch\": epoch+1, **train_metrics, **val_metrics}\n",
    "    all_epoch_metrics.append(epoch_record)\n",
    "\n",
    "    with open(os.path.join(cfg.output_dir, \"epoch_metrics.json\"), \"w\") as f:\n",
    "            json.dump(all_epoch_metrics, f, indent=2)\n",
    "\n",
    "    pd.DataFrame(all_epoch_metrics).to_csv(os.path.join(cfg.output_dir, \"epoch_metrics.csv\"), index=False)\n",
    "\n",
    "    all_epoch_metrics = record_and_save_epoch(\n",
    "        epoch,\n",
    "        train_metrics,\n",
    "        val_metrics,\n",
    "        all_epoch_metrics,\n",
    "        cfg.output_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1526f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime, json\n",
    "\n",
    "summary_path = os.path.join(output_dir, \"training_summary.json\")\n",
    "resume_path = os.path.join(output_dir, \"best_model.pth\")\n",
    "best_model_path = os.path.join(output_dir, f\"best_model.pth\")\n",
    "\n",
    "patience = 5\n",
    "best_epoch = 0\n",
    "start_epoch = 0\n",
    "best_total_acc = 0.0\n",
    "early_stop_counter = 0\n",
    "\n",
    "best_metrics = {}\n",
    "training_summary= {}\n",
    "\n",
    "# Try loading resume state\n",
    "if os.path.exists(resume_path) and os.path.exists(summary_path):\n",
    "    print(f\"🔁 Resuming training from checkpoint and summary\")\n",
    "    \n",
    "    # Load model checkpoint\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_total_acc = checkpoint.get('acc_total', 0.0)\n",
    "    best_metrics = checkpoint.get('metrics', {})\n",
    "\n",
    "    # Load summary info (optional counters/history)\n",
    "    with open(summary_path, \"r\") as f:\n",
    "        summary_data = json.load(f)\n",
    "        early_stop_counter = summary_data.get(\"early_stop_counter\", 0)\n",
    "        acc_energy_list = summary_data.get(\"acc_energy_list\", [])\n",
    "        acc_alpha_list = summary_data.get(\"acc_alpha_list\", [])\n",
    "        acc_q0_list = summary_data.get(\"acc_q0_list\", [])\n",
    "        acc_total_list = summary_data.get(\"acc_total_list\", [])\n",
    "        all_epoch_metrics = summary_data.get(\"all_epoch_metrics\", [])\n",
    "\n",
    "    print(f\"[INFO] Resumed at epoch {start_epoch} with total acc {best_total_acc:.4f} and early stop counter {early_stop_counter}\")\n",
    "else:\n",
    "    print(f\"[INFO] Starting fresh training run\")\n",
    "    \n",
    "    # Initial summary structure\n",
    "    training_summary = {\n",
    "        \"model_tag\": model_tag,\n",
    "        \"backbone\": backbone,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"dataset_root\": dataset_root_dir,\n",
    "        \"global_max\": global_max,\n",
    "        \"start_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"status\": \"interrupted_or_incomplete\"\n",
    "    }\n",
    "\n",
    "    # Save the early config snapshot\n",
    "    summary_path = os.path.join(output_dir, \"training_summary.json\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump(training_summary, f, indent=2)\n",
    "\n",
    "    print(f\"[INFO] Initial training summary saved at: {summary_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de47d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume state\n",
    "start_epoch, best_total_acc, early_stop_counter, best_epoch, best_metrics, all_epoch_metrics = 0, 0.0, 0, 0, {}, []\n",
    "\n",
    "# Training trackers\n",
    "\n",
    "train_loss_energy_list, train_loss_alpha_list, train_loss_q0_list, train_losses = [], [], [],[]\n",
    "train_energy_acc_list, train_alpha_acc_list, train_q0_acc_list, train_total_acc_list = [], [], [], []\n",
    "\n",
    "\n",
    "val_losses,val_loss_energy_list, val_loss_alpha_list,val_loss_q0_list = [], [], [], []\n",
    "acc_energy_list, acc_alpha_list,acc_q0_list ,acc_total_list = [],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4845346",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    train_metrics = train_one_epoch(train_loader, model, criterion, optimizer, device)\n",
    "\n",
    "    # Save metrics\n",
    "    \n",
    "    train_energy_acc_list.append(train_metrics['train_acc_energy'])\n",
    "    train_alpha_acc_list.append(train_metrics['train_acc_alpha'])\n",
    "    train_q0_acc_list.append(train_metrics['train_acc_q0'])\n",
    "    train_total_acc_list.append(train_metrics['train_acc_total'])\n",
    "\n",
    "    train_loss_energy_list.append(train_metrics['train_loss_energy'])\n",
    "    train_loss_alpha_list.append(train_metrics['train_loss_alpha'])\n",
    "    train_loss_q0_list.append(train_metrics['train_loss_q0'])\n",
    "    train_losses.append(train_metrics['train_loss'])\n",
    "    \n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Accuracy: {train_metrics['train_acc_total']:.4f} - Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "\n",
    "    # metrics, acc_total = evaluate(val_loader, model, device)\n",
    "    metrics = evaluate(val_loader, model, criterion, device)\n",
    "\n",
    "    \n",
    "    acc_energy_list.append(metrics['energy']['accuracy'])\n",
    "    acc_alpha_list.append(metrics['alpha']['accuracy'])\n",
    "    acc_q0_list.append(metrics['q0']['accuracy'])\n",
    "    acc_total = metrics[\"total_accuracy\"]\n",
    "    acc_total_list.append(acc_total)\n",
    "    \n",
    "\n",
    "    val_loss = metrics[\"val_loss\"]\n",
    "    val_loss_energy = metrics[\"val_loss_energy\"]\n",
    "    val_loss_alpha = metrics[\"val_loss_alpha\"]\n",
    "    val_loss_q0 = metrics[\"val_loss_q0\"]\n",
    "    val_losses.append(val_loss)\n",
    "    val_loss_energy_list.append(val_loss_energy)\n",
    "    val_loss_alpha_list.append(val_loss_alpha)\n",
    "    val_loss_q0_list.append(val_loss_q0)\n",
    "\n",
    "    \n",
    "    import datetime\n",
    "    print(\"=\"*epochs)\n",
    "    print(f\"Epoch {epoch+1}: Energy Acc ={metrics['energy']['accuracy']:.4f}, αs Acc = {metrics['alpha']['accuracy']:.4f}, Q0 Acc = {metrics['q0']['accuracy']:.4f}, Total Acc = {acc_total:.4f}\")\n",
    "    if acc_total > best_total_acc:\n",
    "        best_total_acc = acc_total\n",
    "        best_metrics = metrics\n",
    "        best_epoch = epoch + 1\n",
    "        early_stop_counter = 0  # reset counter\n",
    "\n",
    "        # Save best model\n",
    "        best_model_path = os.path.join(output_dir, \"best_model.pth\")\n",
    "        torch.save({\n",
    "            'epoch': best_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metrics': best_metrics,\n",
    "            'acc_total': acc_total,\n",
    "            'backbone': backbone,\n",
    "            'input_shape': (1, 32, 32),\n",
    "        }, best_model_path)\n",
    "\n",
    "        print(f\"✅ Best model saved at epoch {best_epoch} with total accuracy: {acc_total:.4f}\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"⏳ No improvement. Early stop counter: {early_stop_counter}/{patience}\")\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"🛑 Early stopping triggered at epoch {epoch+1}. Best was at epoch {best_epoch}.\")\n",
    "            break\n",
    "        \n",
    "    # print(f\"✅ Best model saved with total accuracy: {acc_total:.4f}\")\n",
    "    print(f\"[{datetime.datetime.now().strftime('%H:%M:%S')}] Epoch {epoch+1}/{epochs} - Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "    epoch_record = {\n",
    "    \"epoch\": epoch + 1,\n",
    "\n",
    "    # Train losses\n",
    "    \"train_loss\": train_metrics[\"train_loss\"],\n",
    "    \"train_loss_energy\": train_metrics[\"train_loss_energy\"],\n",
    "    \"train_loss_alpha\": train_metrics[\"train_loss_alpha\"],\n",
    "    \"train_loss_q0\": train_metrics[\"train_loss_q0\"],\n",
    "\n",
    "    # Train accuracies\n",
    "    \"train_acc_energy\": train_metrics[\"train_acc_energy\"],\n",
    "    \"train_acc_alpha\": train_metrics[\"train_acc_alpha\"],\n",
    "    \"train_acc_q0\": train_metrics[\"train_acc_q0\"],\n",
    "    \"train_acc_total\": train_metrics[\"train_acc_total\"],\n",
    "\n",
    "    # Validation loss (from metrics)\n",
    "    \"val_loss\": metrics[\"val_loss\"],\n",
    "    \"val_loss_energy\": metrics[\"val_loss_energy\"],\n",
    "    \"val_loss_alpha\": metrics[\"val_loss_alpha\"],\n",
    "    \"val_loss_q0\": metrics[\"val_loss_q0\"],\n",
    "\n",
    "    # Validation metrics\n",
    "    \"val_total_accuracy\": metrics[\"total_accuracy\"],\n",
    "    \"val_energy\": metrics[\"energy\"],\n",
    "    \"val_alpha\": metrics[\"alpha\"],\n",
    "    \"val_q0\": metrics[\"q0\"],\n",
    "}\n",
    "    all_epoch_metrics.append(epoch_record)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"epoch_metrics.json\"), \"w\") as f:\n",
    "        json.dump(all_epoch_metrics, f, indent=2)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'epoch': m['epoch'],\n",
    "\n",
    "            # Train loss and accuracy\n",
    "            'train_loss': m['train_loss'],\n",
    "            'train_loss_energy': m['train_loss_energy'],\n",
    "            'train_loss_alpha': m['train_loss_alpha'],\n",
    "            'train_loss_q0': m['train_loss_q0'],\n",
    "\n",
    "            'train_acc_energy': m['train_acc_energy'],\n",
    "            'train_acc_alpha': m['train_acc_alpha'],\n",
    "            'train_acc_q0': m['train_acc_q0'],\n",
    "            'train_acc_total': m['train_acc_total'],\n",
    "\n",
    "            # === Validation losses and accuracy ===\n",
    "            'val_loss': m['val_loss'],\n",
    "            'val_loss_energy': m['val_loss_energy'],\n",
    "            'val_loss_alpha': m['val_loss_alpha'],\n",
    "            'val_loss_q0': m['val_loss_q0'],\n",
    "\n",
    "            # Val loss and total acc\n",
    "            'val_total_acc': m['val_total_accuracy'],\n",
    "\n",
    "            # Validation metrics (flattened)\n",
    "            'val_energy_acc': m['val_energy']['accuracy'],\n",
    "            'val_energy_prec': m['val_energy']['precision'],\n",
    "            'val_energy_rec': m['val_energy']['recall'],\n",
    "            'val_energy_f1': m['val_energy']['f1'],\n",
    "\n",
    "            'val_alpha_acc': m['val_alpha']['accuracy'],\n",
    "            'val_alpha_prec': m['val_alpha']['precision'],\n",
    "            'val_alpha_rec': m['val_alpha']['recall'],\n",
    "            'val_alpha_f1': m['val_alpha']['f1'],\n",
    "\n",
    "            'val_q0_acc': m['val_q0']['accuracy'],\n",
    "            'val_q0_prec': m['val_q0']['precision'],\n",
    "            'val_q0_rec': m['val_q0']['recall'],\n",
    "            'val_q0_f1': m['val_q0']['f1'],\n",
    "        \n",
    "        }\n",
    "        for m in all_epoch_metrics\n",
    "    ])\n",
    "    df.to_csv(os.path.join(output_dir, \"epoch_metrics.csv\"), index=False)\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'acc_total': acc_total,\n",
    "        'metrics': metrics,\n",
    "    }, os.path.join(output_dir, f\"checkpoint_epoch_{epoch+1}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update and finalize the training summary\n",
    "training_summary.update({\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_total_accuracy\": best_total_acc,\n",
    "    \"end_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"status\": \"completed\",\n",
    "    \"best_model_path\": os.path.abspath(os.path.join(output_dir, \"best_model.pth\")),\n",
    "    \"metrics_file\": os.path.abspath(os.path.join(output_dir, \"best_model_metrics.json\")),\n",
    "    \"epoch_metrics_csv\": os.path.abspath(os.path.join(output_dir, \"epoch_metrics.csv\")),\n",
    "    \"epoch_metrics_json\": os.path.abspath(os.path.join(output_dir, \"epoch_metrics.json\"))\n",
    "})\n",
    "\n",
    "# Overwrite with final summary\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"✅ Final training summary updated and saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n🏁 Best Model @ Epoch {best_epoch}\")\n",
    "print(f\"Total Accuracy: {best_total_acc:.4f}\")\n",
    "for task in ['energy', 'alpha', 'q0']:\n",
    "    print(f\"\\n🔹 {task.upper()} Task\")\n",
    "    print(f\"  Accuracy : {best_metrics[task]['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_metrics[task]['precision']:.4f}\")\n",
    "    print(f\"  Recall   : {best_metrics[task]['recall']:.4f}\")\n",
    "    print(f\"  F1-Score : {best_metrics[task]['f1']:.4f}\")\n",
    "print(f\"Saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = np.arange(1, len(train_losses) + 1)\n",
    "\n",
    "# Define shared colors for each task\n",
    "colors = {\n",
    "    \"total\": \"black\",\n",
    "    \"energy\": \"red\",\n",
    "    \"alpha\": \"blue\",\n",
    "    \"q0\": \"green\",\n",
    "}\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# === Plot Losses (solid lines) ===\n",
    "ax1.plot(epochs, train_losses, label=\"$Loss_{Total}$\", color=colors[\"total\"], linestyle='-')\n",
    "ax1.plot(epochs, train_loss_energy_list, label=\"$Loss_{Energy}$\", color=colors[\"energy\"], linestyle='-')\n",
    "ax1.plot(epochs, train_loss_alpha_list, label=\"$Loss_{{\\\\alpha}_s}$\", color=colors[\"alpha\"], linestyle='-')\n",
    "ax1.plot(epochs, train_loss_q0_list, label=\"$Loss_{Q_0}$\", color=colors[\"q0\"], linestyle='-')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "# === Plot Accuracies (dashed lines) ===\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, train_total_acc_list, label=\"$Acc_{Total}$\", color=colors[\"total\"], linestyle='--')\n",
    "ax2.plot(epochs, train_energy_acc_list, label=\"$Acc_{Energy}$\", color=colors[\"energy\"], linestyle='--')\n",
    "ax2.plot(epochs, train_alpha_acc_list, label=\"$Acc_{{\\\\alpha}_s}$\", color=colors[\"alpha\"], linestyle='--')\n",
    "ax2.plot(epochs, train_q0_acc_list, label=\"$Acc_{Q_0}$\", color=colors[\"q0\"], linestyle='--')\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Axis labels and title\n",
    "ax1.set_title(\"Train Loss and Accuracy per Epoch \")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Combine Legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save loss plot\n",
    "train_loss_acc_path_png = os.path.join(output_dir, \"train_loss_acc_plot.png\")\n",
    "train_loss_acc_path_pdf = os.path.join(output_dir, \"train_loss_acc_plot.pdf\")\n",
    "\n",
    "plt.savefig(train_loss_acc_path_png, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(train_loss_acc_path_pdf, bbox_inches='tight')\n",
    "\n",
    "print(f\"📉 Loss plot saved as:\\n  - {train_loss_acc_path_png}\\n  - {train_loss_acc_path_pdf}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = np.arange(1, len(val_losses) + 1)\n",
    "\n",
    "# Define shared colors for each task\n",
    "colors = {\n",
    "    \"total\": \"black\",\n",
    "    \"energy\": \"red\",\n",
    "    \"alpha\": \"blue\",\n",
    "    \"q0\": \"green\",\n",
    "}\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# === Plot Validation Losses (solid lines) ===\n",
    "ax1.plot(epochs, val_losses, label=\"$Loss_{Total}$\", color=colors[\"total\"], linestyle='-')\n",
    "ax1.plot(epochs, val_loss_energy_list, label=\"$Loss_{Energy}$\", color=colors[\"energy\"], linestyle='-')\n",
    "ax1.plot(epochs, val_loss_alpha_list, label=\"$Loss_{{\\\\alpha}_s}$\", color=colors[\"alpha\"], linestyle='-')\n",
    "ax1.plot(epochs, val_loss_q0_list, label=\"$Loss_{Q_0}$\", color=colors[\"q0\"], linestyle='-')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "# === Plot Validation Accuracies (dashed lines) ===\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, acc_total_list, label=\"$Acc_{Total}$\", color=colors[\"total\"], linestyle='--')\n",
    "ax2.plot(epochs, acc_energy_list, label=\"$Acc_{Energy}$\", color=colors[\"energy\"], linestyle='--')\n",
    "ax2.plot(epochs, acc_alpha_list, label=\"$Acc_{{\\\\alpha}_s}$\", color=colors[\"alpha\"], linestyle='--')\n",
    "ax2.plot(epochs, acc_q0_list, label=\"$Acc_{Q_0}$\", color=colors[\"q0\"], linestyle='--')\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Axis labels and title\n",
    "ax1.set_title(\"Validation Loss and Accuracy per Epoch\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Combine Legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plots\n",
    "val_loss_acc_path_png = os.path.join(output_dir, \"val_loss_acc_plot.png\")\n",
    "val_loss_acc_path_pdf = os.path.join(output_dir, \"val_loss_acc_plot.pdf\")\n",
    "\n",
    "plt.savefig(val_loss_acc_path_png, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(val_loss_acc_path_pdf, bbox_inches='tight')\n",
    "\n",
    "print(f\"📉 Validation plot saved as:\\n  - {val_loss_acc_path_png}\\n  - {val_loss_acc_path_pdf}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa83f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.model_torch import create_model\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(best_model_path\n",
    ", map_location=torch.device('cpu'))\n",
    "\n",
    "# Recreate model using saved params\n",
    "model, _ = create_model(\n",
    "    backbone=checkpoint['backbone'],\n",
    "    input_shape=checkpoint['input_shape'],\n",
    "    learning_rate=1e-4  # not used when reloading\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"✅ Model loaded from epoch {checkpoint['epoch']} with Total Accuracy: {checkpoint['acc_total']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[INFO] Saving best metrics to {os.path.join(output_dir, 'best_model_metrics.json')}\")\n",
    "\n",
    "import json\n",
    "with open(os.path.join(output_dir, \"best_model_metrics.json\")\n",
    ", \"w\") as f:\n",
    "    json.dump(checkpoint['metrics'], f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
