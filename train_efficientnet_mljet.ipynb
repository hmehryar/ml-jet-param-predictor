{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95811c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "from data.loader import JetDataset, load_split_from_csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define all backbone-model_tag pairs to evaluate ===\n",
    "experiments = [\n",
    "    {\"model_tag\": \"EfficientNet\", \"backbone\": \"efficientnet\"},\n",
    "    {\"model_tag\": \"ConvNeXt\", \"backbone\": \"convnext\"},\n",
    "    {\"model_tag\": \"SwinTransformerV2\", \"backbone\": \"swin\"},\n",
    "    {\"model_tag\": \"Mamba\", \"backbone\": \"mamba\"},\n",
    "    {\"model_tag\": \"VisionMamba\", \"backbone\": \"vision_mamba\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ad3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_idx = 0\n",
    "model_tag = experiments[model_idx][\"model_tag\"]\n",
    "backbone = experiments[model_idx][\"backbone\"]\n",
    "# ==========================================================\n",
    "print(f\"Evaluating {model_tag} with backbone {backbone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3dccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "global_max = 121.79151153564453\n",
    "output_dir = 'training_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import getpass\n",
    "\n",
    "# === Determine platform-specific base path ===\n",
    "#--- System/User Info ---\n",
    "system = platform.system()\n",
    "release = platform.release().lower()\n",
    "hostname = socket.gethostname().lower()\n",
    "user = getpass.getuser().lower()\n",
    "\n",
    "# === Base path detection ===\n",
    "if system == \"Linux\" and \"wsl\" in platform.release().lower() and  \"arsalan\" in user:\n",
    "    base_path = \"/mnt/d/Projects/110_JetscapeML/hm_jetscapeml_source/data\"\n",
    "    print(\"[INFO] Detected WSL environment\")\n",
    "elif system == \"Linux\" and \"ds044955\" in hostname and \"arsalan\" in user:\n",
    "    base_path = \"/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data\"\n",
    "    print(\"[INFO] Detected native Ubuntu host: DS044955\")\n",
    "elif system == \"Linux\" and  \"gy4065\" in user:\n",
    "    base_path = \"/wsu/home/gy/gy40/gy4065/hm_jetscapeml_source/data\"\n",
    "    print(\"[INFO] Detected WSU Grid environment (user: gy4065)\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå Unknown system. Please define the dataset path for this host.\")\n",
    "\n",
    "# === Define dataset subdirectory (only this part changes per dataset) ===\n",
    "dataset_subdir = \"jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled\"\n",
    "\n",
    "# === Full dataset path ===\n",
    "dataset_root_dir = os.path.join(base_path, dataset_subdir)\n",
    "print(f\"[INFO] Using dataset root: {dataset_root_dir}\")\n",
    "\n",
    "# === Extract dataset size from path ===\n",
    "match = re.search(r\"size_(\\d+)\", dataset_root_dir)\n",
    "dataset_size = match.group(1) if match else \"unknown\"\n",
    "print(f\"[INFO] Detected dataset size: {dataset_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dynamic output directory\n",
    "run_tag = f\"{model_tag}_bs{batch_size}_ep{epochs}_lr{learning_rate:.0e}_ds{dataset_size}\"\n",
    "output_dir = os.path.join(output_dir, run_tag)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"[INFO] Saving all outputs to: {output_dir}\")\n",
    "\n",
    "train_csv = os.path.join(dataset_root_dir, 'train_files.csv')\n",
    "val_csv = os.path.join(dataset_root_dir, 'val_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splits from CSV\n",
    "train_list = load_split_from_csv(train_csv, dataset_root_dir)\n",
    "val_list = load_split_from_csv(val_csv, dataset_root_dir)\n",
    "\n",
    "train_dataset = JetDataset(train_list, global_max=global_max)\n",
    "val_dataset = JetDataset(val_list, global_max=global_max)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"[INFO] Training samples: {len(train_dataset)}\")\n",
    "print(f\"[INFO] Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "#show the length of the dataloaders\n",
    "print(f\"[INFO] Length of training dataloader: {len(train_loader)}\")\n",
    "print(f\"[INFO] Length of validation dataloader: {len(val_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e838b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "model, optimizer = create_model(backbone=backbone, input_shape=(1, 32, 32), learning_rate=learning_rate)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = {\n",
    "    'energy_loss_output': nn.BCELoss(),\n",
    "    'alpha_output': nn.CrossEntropyLoss(),\n",
    "    'q0_output': nn.CrossEntropyLoss()\n",
    "}\n",
    "print(criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_energy_list, train_loss_alpha_list, train_loss_q0_list, train_losses = [], [], [],[]\n",
    "train_energy_acc_list, train_alpha_acc_list, train_q0_acc_list, train_total_acc_list = [], [], [], []\n",
    "\n",
    "\n",
    "val_losses,val_loss_energy_list, val_loss_alpha_list,val_loss_q0_list = [], [], [], []\n",
    "acc_energy_list, acc_alpha_list,acc_q0_list ,acc_total_list = [],[],[],[]\n",
    "\n",
    "\n",
    "all_epoch_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f0ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected native Ubuntu host: DS044955\n",
      "[INFO] Using dataset root: /home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled\n",
      "[INFO] Detected dataset size: 1000\n",
      "[INFO] model tag: EfficientNet, backbone: efficientnet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_tag': 'EfficientNet',\n",
       " 'backbone': 'efficientnet',\n",
       " 'batch_size': 512,\n",
       " 'epochs': 50,\n",
       " 'learning_rate': 0.0001,\n",
       " 'patience': (5,),\n",
       " 'input_shape': (1, 32, 32),\n",
       " 'global_max': 121.79151153564453,\n",
       " 'dataset_root_dir': '/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled',\n",
       " 'train_csv': '/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled/train_files.csv',\n",
       " 'val_csv': '/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled/val_files.csv',\n",
       " 'test_csv': '/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_balanced_unshuffled/test_files.csv',\n",
       " 'output_dir': 'training_output/EfficientNet_bs512_ep50_lr1e-04_ds1000'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving all outputs to: training_output/EfficientNet_bs512_ep50_lr1e-04_ds1000\n",
      "[INFO] Training samples: 796\n",
      "[INFO] Validation samples: 96\n",
      "[INFO] Test samples: 108\n",
      "[INFO] Length of training dataloader: 2\n",
      "[INFO] Length of validation dataloader: 1\n",
      "[INFO] Length of test dataloader: 1\n",
      "[INFO] Loss functions:{'energy_loss_output': BCELoss(), 'alpha_output': CrossEntropyLoss(), 'q0_output': CrossEntropyLoss()}\n",
      "[INFO] Init Resume Trackers\n",
      "[INFO] Init Training Trackers\n",
      "[INFO] Init Training Trackers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from config import get_config\n",
    "from data.loader import get_dataloaders\n",
    "from models.model import create_model\n",
    "\n",
    "cfg=get_config()\n",
    "display(cfg)\n",
    "\n",
    "os.makedirs(cfg['output_dir'], exist_ok=True)\n",
    "print(f\"[INFO] Saving all outputs to: {cfg['output_dir']}\")\n",
    "\n",
    "\n",
    "# Set seed, device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data\n",
    "train_loader, val_loader, test_loader = get_dataloaders(cfg)\n",
    "\n",
    "# Model and optimizer\n",
    "model, optimizer = create_model(cfg['backbone'], cfg['input_shape'], cfg['learning_rate'])\n",
    "model.to(device)\n",
    "\n",
    "criterion = {\n",
    "    'energy_loss_output': nn.BCELoss(),\n",
    "    'alpha_output': nn.CrossEntropyLoss(),\n",
    "    'q0_output': nn.CrossEntropyLoss()\n",
    "}\n",
    "print(f\"[INFO] Loss functions:{criterion}\")\n",
    "\n",
    "# Resume trackers\n",
    "print(f\"[INFO] Init Resume Trackers\")\n",
    "all_epoch_metrics = []\n",
    "\n",
    "# Training trackers\n",
    "print(f\"[INFO] Init Training Trackers\")\n",
    "train_loss_energy_list, train_loss_alpha_list, train_loss_q0_list, train_losses = [], [], [],[]\n",
    "train_energy_acc_list, train_alpha_acc_list, train_q0_acc_list, train_total_acc_list = [], [], [], []\n",
    "\n",
    "#Validation trackers\n",
    "print(f\"[INFO] Init Training Trackers\")\n",
    "val_losses,val_loss_energy_list, val_loss_alpha_list,val_loss_q0_list = [], [], [], []\n",
    "acc_energy_list, acc_alpha_list,acc_q0_list ,acc_total_list = [],[],[],[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af732c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    loss_energy_total = 0.0\n",
    "    loss_alpha_total = 0.0\n",
    "    loss_q0_total = 0.0\n",
    "    \n",
    "    correct_energy = 0\n",
    "    correct_alpha = 0\n",
    "    correct_q0 = 0\n",
    "    correct_all = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x = x.to(device)\n",
    "        for key in labels:\n",
    "            labels[key] = labels[key].to(device)\n",
    "         # Forward\n",
    "        outputs = model(x)\n",
    "        energy_out = outputs['energy_loss_output'].squeeze()\n",
    "        alpha_out = outputs['alpha_output']\n",
    "        q0_out = outputs['q0_output']\n",
    "\n",
    "        # Labels\n",
    "        gt_energy = labels['energy_loss_output'].squeeze()\n",
    "        gt_alpha = labels['alpha_output'].squeeze()\n",
    "        gt_q0 = labels['q0_output'].squeeze()\n",
    "\n",
    "        # Loss\n",
    "        loss_energy = criterion['energy_loss_output'](energy_out, gt_energy.float())\n",
    "        loss_alpha = criterion['alpha_output'](alpha_out, gt_alpha)\n",
    "        loss_q0 = criterion['q0_output'](q0_out, gt_q0)\n",
    "        total_batch_loss = loss_energy + loss_alpha + loss_q0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # running_loss += loss.item()\n",
    "        total_loss += total_batch_loss.item()\n",
    "        loss_energy_total += loss_energy.item()\n",
    "        loss_alpha_total += loss_alpha.item()\n",
    "        loss_q0_total += loss_q0.item()\n",
    "\n",
    "        # Accuracy\n",
    "        pred_energy = (energy_out > 0.5).long()\n",
    "        pred_alpha = torch.argmax(alpha_out, dim=1)\n",
    "        pred_q0 = torch.argmax(q0_out, dim=1)\n",
    "\n",
    "        correct_energy += (pred_energy == gt_energy).sum().item()\n",
    "        correct_alpha += (pred_alpha == gt_alpha).sum().item()\n",
    "        correct_q0 += (pred_q0 == gt_q0).sum().item()\n",
    "\n",
    "        correct_all += ((pred_energy == gt_energy) &\n",
    "                        (pred_alpha == gt_alpha) &\n",
    "                        (pred_q0 == gt_q0)).sum().item()\n",
    "\n",
    "        total += x.size(0)\n",
    "\n",
    "    return {\n",
    "        'train_loss': total_loss / len(loader),\n",
    "        'train_loss_energy': loss_energy_total / len(loader),\n",
    "        'train_loss_alpha': loss_alpha_total / len(loader),\n",
    "        'train_loss_q0': loss_q0_total / len(loader),\n",
    "        'train_acc_energy': correct_energy / total,\n",
    "        'train_acc_alpha': correct_alpha / total,\n",
    "        'train_acc_q0': correct_q0 / total,\n",
    "        'train_acc_total': correct_all / total\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ccf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    y_true = {'energy': [], 'alpha': [], 'q0': []}\n",
    "    y_pred = {'energy': [], 'alpha': [], 'q0': []}\n",
    "    correct_all = 0\n",
    "    total = 0\n",
    "\n",
    "    val_loss_total = 0.0\n",
    "    val_loss_energy = 0.0\n",
    "    val_loss_alpha = 0.0\n",
    "    val_loss_q0 = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, labels in loader:\n",
    "            x = x.to(device)\n",
    "            for key in labels:\n",
    "                labels[key] = labels[key].to(device)\n",
    "\n",
    "            outputs = model(x)\n",
    "\n",
    "            # Energy loss: binary thresholding\n",
    "            pred_energy = (outputs['energy_loss_output'] > 0.5).long().squeeze()\n",
    "            pred_alpha = torch.argmax(outputs['alpha_output'], dim=1)\n",
    "            pred_q0 = torch.argmax(outputs['q0_output'], dim=1)\n",
    "\n",
    "            gt_energy = labels['energy_loss_output'].squeeze()\n",
    "            gt_alpha = labels['alpha_output'].squeeze()\n",
    "            gt_q0 = labels['q0_output'].squeeze()\n",
    "\n",
    "            y_true['energy'].extend(labels['energy_loss_output'].squeeze().cpu().numpy())\n",
    "            y_true['alpha'].extend(labels['alpha_output'].squeeze().cpu().numpy())\n",
    "            y_true['q0'].extend(labels['q0_output'].squeeze().cpu().numpy())\n",
    "\n",
    "            y_pred['energy'].extend(pred_energy.cpu().numpy())\n",
    "            y_pred['alpha'].extend(pred_alpha.cpu().numpy())\n",
    "            y_pred['q0'].extend(pred_q0.cpu().numpy())\n",
    "\n",
    "            # Total accuracy = all 3 correct\n",
    "            correct_batch = ((pred_energy == gt_energy) &\n",
    "                             (pred_alpha == gt_alpha) &\n",
    "                             (pred_q0 == gt_q0)).sum().item()\n",
    "            correct_all += correct_batch\n",
    "            total += x.size(0)\n",
    "\n",
    "            # Compute loss per task\n",
    "            energy_out = outputs['energy_loss_output'].squeeze()\n",
    "            alpha_out = outputs['alpha_output']\n",
    "            q0_out = outputs['q0_output']\n",
    "\n",
    "            loss_energy = criterion['energy_loss_output'](energy_out, gt_energy.float())\n",
    "            loss_alpha = criterion['alpha_output'](alpha_out, gt_alpha)\n",
    "            loss_q0 = criterion['q0_output'](q0_out, gt_q0)\n",
    "\n",
    "            val_loss_energy += loss_energy.item()\n",
    "            val_loss_alpha += loss_alpha.item()\n",
    "            val_loss_q0 += loss_q0.item()\n",
    "            val_loss_total += (loss_energy + loss_alpha + loss_q0).item()\n",
    "\n",
    "            \n",
    "    # Compute individual accuracies\n",
    "    acc_total = correct_all / total\n",
    "    avg_loss_energy = val_loss_energy / len(loader)\n",
    "    avg_loss_alpha = val_loss_alpha / len(loader)\n",
    "    avg_loss_q0 = val_loss_q0 / len(loader)\n",
    "    avg_val_loss = val_loss_total / len(loader)\n",
    "    \n",
    "    # All metrics + losses in one dict\n",
    "    metrics = {\n",
    "        \"total_accuracy\": acc_total,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"val_loss_energy\": avg_loss_energy,\n",
    "        \"val_loss_alpha\": avg_loss_alpha,\n",
    "        \"val_loss_q0\": avg_loss_q0,\n",
    "        \"energy\": {\n",
    "            \"accuracy\": accuracy_score(y_true['energy'], y_pred['energy']),\n",
    "            \"precision\": precision_score(y_true['energy'], y_pred['energy'], average='macro'),\n",
    "            \"recall\": recall_score(y_true['energy'], y_pred['energy'], average='macro'),\n",
    "            \"f1\": f1_score(y_true['energy'], y_pred['energy'], average='macro'),\n",
    "        },\n",
    "        \"alpha\": {\n",
    "            \"accuracy\": accuracy_score(y_true['alpha'], y_pred['alpha']),\n",
    "            \"precision\": precision_score(y_true['alpha'], y_pred['alpha'], average='macro'),\n",
    "            \"recall\": recall_score(y_true['alpha'], y_pred['alpha'], average='macro'),\n",
    "            \"f1\": f1_score(y_true['alpha'], y_pred['alpha'], average='macro'),\n",
    "        },\n",
    "        \"q0\": {\n",
    "            \"accuracy\": accuracy_score(y_true['q0'], y_pred['q0']),\n",
    "            \"precision\": precision_score(y_true['q0'], y_pred['q0'], average='macro'),\n",
    "            \"recall\": recall_score(y_true['q0'], y_pred['q0'], average='macro'),\n",
    "            \"f1\": f1_score(y_true['q0'], y_pred['q0'], average='macro'),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de47d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume state\n",
    "start_epoch, best_total_acc, early_stop_counter, best_epoch, best_metrics, all_epoch_metrics = 0, 0.0, 0, 0, {}, []\n",
    "\n",
    "# Training trackers\n",
    "\n",
    "train_loss_energy_list, train_loss_alpha_list, train_loss_q0_list, train_losses = [], [], [],[]\n",
    "train_energy_acc_list, train_alpha_acc_list, train_q0_acc_list, train_total_acc_list = [], [], [], []\n",
    "\n",
    "\n",
    "val_losses,val_loss_energy_list, val_loss_alpha_list,val_loss_q0_list = [], [], [], []\n",
    "acc_energy_list, acc_alpha_list,acc_q0_list ,acc_total_list = [],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1526f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime, json\n",
    "\n",
    "summary_path = os.path.join(output_dir, \"training_summary.json\")\n",
    "resume_path = os.path.join(output_dir, \"best_model.pth\")\n",
    "best_model_path = os.path.join(output_dir, f\"best_model.pth\")\n",
    "\n",
    "patience = 5\n",
    "best_epoch = 0\n",
    "start_epoch = 0\n",
    "best_total_acc = 0.0\n",
    "early_stop_counter = 0\n",
    "\n",
    "best_metrics = {}\n",
    "training_summary= {}\n",
    "\n",
    "# Try loading resume state\n",
    "if os.path.exists(resume_path) and os.path.exists(summary_path):\n",
    "    print(f\"üîÅ Resuming training from checkpoint and summary\")\n",
    "    \n",
    "    # Load model checkpoint\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_total_acc = checkpoint.get('acc_total', 0.0)\n",
    "    best_metrics = checkpoint.get('metrics', {})\n",
    "\n",
    "    # Load summary info (optional counters/history)\n",
    "    with open(summary_path, \"r\") as f:\n",
    "        summary_data = json.load(f)\n",
    "        early_stop_counter = summary_data.get(\"early_stop_counter\", 0)\n",
    "        acc_energy_list = summary_data.get(\"acc_energy_list\", [])\n",
    "        acc_alpha_list = summary_data.get(\"acc_alpha_list\", [])\n",
    "        acc_q0_list = summary_data.get(\"acc_q0_list\", [])\n",
    "        acc_total_list = summary_data.get(\"acc_total_list\", [])\n",
    "        all_epoch_metrics = summary_data.get(\"all_epoch_metrics\", [])\n",
    "\n",
    "    print(f\"[INFO] Resumed at epoch {start_epoch} with total acc {best_total_acc:.4f} and early stop counter {early_stop_counter}\")\n",
    "else:\n",
    "    print(f\"[INFO] Starting fresh training run\")\n",
    "    \n",
    "    # Initial summary structure\n",
    "    training_summary = {\n",
    "        \"model_tag\": model_tag,\n",
    "        \"backbone\": backbone,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"dataset_root\": dataset_root_dir,\n",
    "        \"global_max\": global_max,\n",
    "        \"start_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"status\": \"interrupted_or_incomplete\"\n",
    "    }\n",
    "\n",
    "    # Save the early config snapshot\n",
    "    summary_path = os.path.join(output_dir, \"training_summary.json\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump(training_summary, f, indent=2)\n",
    "\n",
    "    print(f\"[INFO] Initial training summary saved at: {summary_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4845346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    train_metrics = train_one_epoch(train_loader, model, criterion, optimizer, device)\n",
    "\n",
    "    # Save metrics\n",
    "    \n",
    "    train_energy_acc_list.append(train_metrics['train_acc_energy'])\n",
    "    train_alpha_acc_list.append(train_metrics['train_acc_alpha'])\n",
    "    train_q0_acc_list.append(train_metrics['train_acc_q0'])\n",
    "    train_total_acc_list.append(train_metrics['train_acc_total'])\n",
    "\n",
    "    train_loss_energy_list.append(train_metrics['train_loss_energy'])\n",
    "    train_loss_alpha_list.append(train_metrics['train_loss_alpha'])\n",
    "    train_loss_q0_list.append(train_metrics['train_loss_q0'])\n",
    "    train_losses.append(train_metrics['train_loss'])\n",
    "    \n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Accuracy: {train_metrics['train_acc_total']:.4f} - Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "\n",
    "    # metrics, acc_total = evaluate(val_loader, model, device)\n",
    "    metrics = evaluate(val_loader, model, criterion, device)\n",
    "\n",
    "    acc_total = metrics[\"total_accuracy\"]\n",
    "    \n",
    "    acc_energy_list.append(metrics['energy']['accuracy'])\n",
    "    acc_alpha_list.append(metrics['alpha']['accuracy'])\n",
    "    acc_q0_list.append(metrics['q0']['accuracy'])\n",
    "    acc_total_list.append(acc_total)\n",
    "    \n",
    "\n",
    "    val_loss = metrics[\"val_loss\"]\n",
    "    val_loss_energy = metrics[\"val_loss_energy\"]\n",
    "    val_loss_alpha = metrics[\"val_loss_alpha\"]\n",
    "    val_loss_q0 = metrics[\"val_loss_q0\"]\n",
    "    val_losses.append(val_loss)\n",
    "    val_loss_energy_list.append(val_loss_energy)\n",
    "    val_loss_alpha_list.append(val_loss_alpha)\n",
    "    val_loss_q0_list.append(val_loss_q0)\n",
    "\n",
    "    \n",
    "    import datetime\n",
    "    print(\"=\"*epochs)\n",
    "    print(f\"Epoch {epoch+1}: Energy Acc ={metrics['energy']['accuracy']:.4f}, Œ±s Acc = {metrics['alpha']['accuracy']:.4f}, Q0 Acc = {metrics['q0']['accuracy']:.4f}, Total Acc = {acc_total:.4f}\")\n",
    "    if acc_total > best_total_acc:\n",
    "        best_total_acc = acc_total\n",
    "        best_metrics = metrics\n",
    "        best_epoch = epoch + 1\n",
    "        early_stop_counter = 0  # reset counter\n",
    "\n",
    "        # Save best model\n",
    "        best_model_path = os.path.join(output_dir, \"best_model.pth\")\n",
    "        torch.save({\n",
    "            'epoch': best_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metrics': best_metrics,\n",
    "            'acc_total': acc_total,\n",
    "            'backbone': backbone,\n",
    "            'input_shape': (1, 32, 32),\n",
    "        }, best_model_path)\n",
    "\n",
    "        print(f\"‚úÖ Best model saved at epoch {best_epoch} with total accuracy: {acc_total:.4f}\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"‚è≥ No improvement. Early stop counter: {early_stop_counter}/{patience}\")\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"üõë Early stopping triggered at epoch {epoch+1}. Best was at epoch {best_epoch}.\")\n",
    "            break\n",
    "        \n",
    "    # print(f\"‚úÖ Best model saved with total accuracy: {acc_total:.4f}\")\n",
    "    print(f\"[{datetime.datetime.now().strftime('%H:%M:%S')}] Epoch {epoch+1}/{epochs} - Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "    epoch_record = {\n",
    "    \"epoch\": epoch + 1,\n",
    "\n",
    "    # Train losses\n",
    "    \"train_loss\": train_metrics[\"train_loss\"],\n",
    "    \"train_loss_energy\": train_metrics[\"train_loss_energy\"],\n",
    "    \"train_loss_alpha\": train_metrics[\"train_loss_alpha\"],\n",
    "    \"train_loss_q0\": train_metrics[\"train_loss_q0\"],\n",
    "\n",
    "    # Train accuracies\n",
    "    \"train_acc_energy\": train_metrics[\"train_acc_energy\"],\n",
    "    \"train_acc_alpha\": train_metrics[\"train_acc_alpha\"],\n",
    "    \"train_acc_q0\": train_metrics[\"train_acc_q0\"],\n",
    "    \"train_acc_total\": train_metrics[\"train_acc_total\"],\n",
    "\n",
    "    # Validation loss (from metrics)\n",
    "    \"val_loss\": metrics[\"val_loss\"],\n",
    "    \"val_loss_energy\": metrics[\"val_loss_energy\"],\n",
    "    \"val_loss_alpha\": metrics[\"val_loss_alpha\"],\n",
    "    \"val_loss_q0\": metrics[\"val_loss_q0\"],\n",
    "\n",
    "    # Validation metrics\n",
    "    \"val_total_accuracy\": metrics[\"total_accuracy\"],\n",
    "    \"val_energy\": metrics[\"energy\"],\n",
    "    \"val_alpha\": metrics[\"alpha\"],\n",
    "    \"val_q0\": metrics[\"q0\"],\n",
    "}\n",
    "    all_epoch_metrics.append(epoch_record)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"epoch_metrics.json\"), \"w\") as f:\n",
    "        json.dump(all_epoch_metrics, f, indent=2)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'epoch': m['epoch'],\n",
    "\n",
    "            # Train loss and accuracy\n",
    "            'train_loss': m['train_loss'],\n",
    "            'train_loss_energy': m['train_loss_energy'],\n",
    "            'train_loss_alpha': m['train_loss_alpha'],\n",
    "            'train_loss_q0': m['train_loss_q0'],\n",
    "\n",
    "            'train_acc_energy': m['train_acc_energy'],\n",
    "            'train_acc_alpha': m['train_acc_alpha'],\n",
    "            'train_acc_q0': m['train_acc_q0'],\n",
    "            'train_acc_total': m['train_acc_total'],\n",
    "\n",
    "            # === Validation losses and accuracy ===\n",
    "            'val_loss': m['val_loss'],\n",
    "            'val_loss_energy': m['val_loss_energy'],\n",
    "            'val_loss_alpha': m['val_loss_alpha'],\n",
    "            'val_loss_q0': m['val_loss_q0'],\n",
    "\n",
    "            # Val loss and total acc\n",
    "            'val_total_acc': m['val_total_accuracy'],\n",
    "\n",
    "            # Validation metrics (flattened)\n",
    "            'val_energy_acc': m['val_energy']['accuracy'],\n",
    "            'val_energy_prec': m['val_energy']['precision'],\n",
    "            'val_energy_rec': m['val_energy']['recall'],\n",
    "            'val_energy_f1': m['val_energy']['f1'],\n",
    "\n",
    "            'val_alpha_acc': m['val_alpha']['accuracy'],\n",
    "            'val_alpha_prec': m['val_alpha']['precision'],\n",
    "            'val_alpha_rec': m['val_alpha']['recall'],\n",
    "            'val_alpha_f1': m['val_alpha']['f1'],\n",
    "\n",
    "            'val_q0_acc': m['val_q0']['accuracy'],\n",
    "            'val_q0_prec': m['val_q0']['precision'],\n",
    "            'val_q0_rec': m['val_q0']['recall'],\n",
    "            'val_q0_f1': m['val_q0']['f1'],\n",
    "        \n",
    "        }\n",
    "        for m in all_epoch_metrics\n",
    "    ])\n",
    "    df.to_csv(os.path.join(output_dir, \"epoch_metrics.csv\"), index=False)\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'acc_total': acc_total,\n",
    "        'metrics': metrics,\n",
    "    }, os.path.join(output_dir, f\"checkpoint_epoch_{epoch+1}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update and finalize the training summary\n",
    "training_summary.update({\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_total_accuracy\": best_total_acc,\n",
    "    \"end_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"status\": \"completed\",\n",
    "    \"best_model_path\": os.path.abspath(os.path.join(output_dir, \"best_model.pth\")),\n",
    "    \"metrics_file\": os.path.abspath(os.path.join(output_dir, \"best_model_metrics.json\")),\n",
    "    \"epoch_metrics_csv\": os.path.abspath(os.path.join(output_dir, \"epoch_metrics.csv\")),\n",
    "    \"epoch_metrics_json\": os.path.abspath(os.path.join(output_dir, \"epoch_metrics.json\"))\n",
    "})\n",
    "\n",
    "# Overwrite with final summary\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Final training summary updated and saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüèÅ Best Model @ Epoch {best_epoch}\")\n",
    "print(f\"Total Accuracy: {best_total_acc:.4f}\")\n",
    "for task in ['energy', 'alpha', 'q0']:\n",
    "    print(f\"\\nüîπ {task.upper()} Task\")\n",
    "    print(f\"  Accuracy : {best_metrics[task]['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_metrics[task]['precision']:.4f}\")\n",
    "    print(f\"  Recall   : {best_metrics[task]['recall']:.4f}\")\n",
    "    print(f\"  F1-Score : {best_metrics[task]['f1']:.4f}\")\n",
    "print(f\"Saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = np.arange(1, len(train_losses) + 1)\n",
    "\n",
    "# Define shared colors for each task\n",
    "colors = {\n",
    "    \"total\": \"black\",\n",
    "    \"energy\": \"red\",\n",
    "    \"alpha\": \"blue\",\n",
    "    \"q0\": \"green\",\n",
    "}\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# === Plot Losses (solid lines) ===\n",
    "ax1.plot(epochs, train_losses, label=\"$Loss_{Total}$\", color=colors[\"total\"], linestyle='-')\n",
    "ax1.plot(epochs, train_loss_energy_list, label=\"$Loss_{Energy}$\", color=colors[\"energy\"], linestyle='-')\n",
    "ax1.plot(epochs, train_loss_alpha_list, label=\"$Loss_{{\\\\alpha}_s}$\", color=colors[\"alpha\"], linestyle='-')\n",
    "ax1.plot(epochs, train_loss_q0_list, label=\"$Loss_{Q_0}$\", color=colors[\"q0\"], linestyle='-')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "# === Plot Accuracies (dashed lines) ===\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, train_total_acc_list, label=\"$Acc_{Total}$\", color=colors[\"total\"], linestyle='--')\n",
    "ax2.plot(epochs, train_energy_acc_list, label=\"$Acc_{Energy}$\", color=colors[\"energy\"], linestyle='--')\n",
    "ax2.plot(epochs, train_alpha_acc_list, label=\"$Acc_{{\\\\alpha}_s}$\", color=colors[\"alpha\"], linestyle='--')\n",
    "ax2.plot(epochs, train_q0_acc_list, label=\"$Acc_{Q_0}$\", color=colors[\"q0\"], linestyle='--')\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Axis labels and title\n",
    "ax1.set_title(\"Train Loss and Accuracy per Epoch \")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Combine Legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save loss plot\n",
    "train_loss_acc_path_png = os.path.join(output_dir, \"train_loss_acc_plot.png\")\n",
    "train_loss_acc_path_pdf = os.path.join(output_dir, \"train_loss_acc_plot.pdf\")\n",
    "\n",
    "plt.savefig(train_loss_acc_path_png, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(train_loss_acc_path_pdf, bbox_inches='tight')\n",
    "\n",
    "print(f\"üìâ Loss plot saved as:\\n  - {train_loss_acc_path_png}\\n  - {train_loss_acc_path_pdf}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = np.arange(1, len(val_losses) + 1)\n",
    "\n",
    "# Define shared colors for each task\n",
    "colors = {\n",
    "    \"total\": \"black\",\n",
    "    \"energy\": \"red\",\n",
    "    \"alpha\": \"blue\",\n",
    "    \"q0\": \"green\",\n",
    "}\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# === Plot Validation Losses (solid lines) ===\n",
    "ax1.plot(epochs, val_losses, label=\"$Loss_{Total}$\", color=colors[\"total\"], linestyle='-')\n",
    "ax1.plot(epochs, val_loss_energy_list, label=\"$Loss_{Energy}$\", color=colors[\"energy\"], linestyle='-')\n",
    "ax1.plot(epochs, val_loss_alpha_list, label=\"$Loss_{{\\\\alpha}_s}$\", color=colors[\"alpha\"], linestyle='-')\n",
    "ax1.plot(epochs, val_loss_q0_list, label=\"$Loss_{Q_0}$\", color=colors[\"q0\"], linestyle='-')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "# === Plot Validation Accuracies (dashed lines) ===\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, acc_total_list, label=\"$Acc_{Total}$\", color=colors[\"total\"], linestyle='--')\n",
    "ax2.plot(epochs, acc_energy_list, label=\"$Acc_{Energy}$\", color=colors[\"energy\"], linestyle='--')\n",
    "ax2.plot(epochs, acc_alpha_list, label=\"$Acc_{{\\\\alpha}_s}$\", color=colors[\"alpha\"], linestyle='--')\n",
    "ax2.plot(epochs, acc_q0_list, label=\"$Acc_{Q_0}$\", color=colors[\"q0\"], linestyle='--')\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Axis labels and title\n",
    "ax1.set_title(\"Validation Loss and Accuracy per Epoch\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Combine Legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plots\n",
    "val_loss_acc_path_png = os.path.join(output_dir, \"val_loss_acc_plot.png\")\n",
    "val_loss_acc_path_pdf = os.path.join(output_dir, \"val_loss_acc_plot.pdf\")\n",
    "\n",
    "plt.savefig(val_loss_acc_path_png, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(val_loss_acc_path_pdf, bbox_inches='tight')\n",
    "\n",
    "print(f\"üìâ Validation plot saved as:\\n  - {val_loss_acc_path_png}\\n  - {val_loss_acc_path_pdf}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa83f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.model_torch import create_model\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(best_model_path\n",
    ", map_location=torch.device('cpu'))\n",
    "\n",
    "# Recreate model using saved params\n",
    "model, _ = create_model(\n",
    "    backbone=checkpoint['backbone'],\n",
    "    input_shape=checkpoint['input_shape'],\n",
    "    learning_rate=1e-4  # not used when reloading\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Model loaded from epoch {checkpoint['epoch']} with Total Accuracy: {checkpoint['acc_total']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[INFO] Saving best metrics to {os.path.join(output_dir, 'best_model_metrics.json')}\")\n",
    "\n",
    "import json\n",
    "with open(os.path.join(output_dir, \"best_model_metrics.json\")\n",
    ", \"w\") as f:\n",
    "    json.dump(checkpoint['metrics'], f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
